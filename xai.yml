categories:
- AI
- artificial intelligence
- profile
- computers
- science
- food
- sports
- psychology
- trivia
- politics
- movies
- history
- health
- money
- gossip
- conversations
- emotion

conversations:
- - LIME
  - LIME is model-agnostic, meaning that it can be applied to any machine learning model. More details (https://arxiv.org/abs/1911.02508)
  - Model-specific approaches aim to understand the black model machine learning model by analysing the internal components and how they interact. More details (https://arxiv.org/abs/1911.02508)
  - In deep learning models, it is e.g. possible to investigate activation units and to link internal activations back to the input. More details (https://arxiv.org/abs/1911.02508)
  - This requires a thorough understanding of the network and doesn’t scale to other models. More details (https://arxiv.org/abs/1911.02508)
- - What is LIME?
  - The technique attempts to understand the model by perturbing the input of data samples and understanding how the predictions change. More details (https://arxiv.org/abs/1911.02508)
- - SHAP
  - SHAP stands for SHapley Additive exPlanations. More details (https://arxiv.org/abs/1911.02508)
  - In effect it means that the prediction made by the model is calculated for all the subsets with and without the feature and summed to get the Shapley value for that feature.
  - Applying the Shapley’s properties of fairness from the game theory to explainable AI, the below comparison can be derived. More details (https://arxiv.org/abs/1911.02508)
- - frameworks
  - There are many XAI frameworks available amongst which LIME & SHAP are the popular ones.
- - Shapley Values
  - The Shapley value is a solution concept in the cooperative game theory.
- - Neural Graph Learning
  - Label propagation is a powerful and flexible semi-supervised learning technique on graphs. More details (https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf)
  - Neural networks, on the other hand, have proven track records in many supervised learning tasks. More information (https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf)
  - I propose a training framework with a graph-regularized objective, namely Neural Graph Machines, that can combine the power of neural networks and label propagation. More information (https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf)
  - Enabling it to be applied to multiple neural architectures, Feed-forward NNs, CNNs and LSTM RNNs
- - What is Graph Neural Network?
  - Graph Neural Network has gained increasing popularity in various domains, including social network, knowledge graph, recommender system, and even life science.
  - The power of GNN in modeling the dependencies between nodes in a graph enables the breakthrough in the research area related to graph analysis.
  - Let’s first understand what is Graph.
- - What is Graph?
  - A graph is a data structure consisting of two components, vertices and edges.
- - What is GNN?
  - Graph Neural Network is a type of Neural Network which directly operates on the Graph structure.
- - GNN
  - A typical application of GNN is node classification. Essentially, every node in the graph is associated with a label, and we want to predict the label of the nodes without ground-truth.
- - CML
  - Let's subsume learning approaches that efficiently combine human intelligence with the machine’s ability of rapid computation under the term Cooperative Machine Learning.
  - An active learning module then decides which parts of the prediction are subject to manual revision by human annotators.
  - I propose a novel two-step CML strategy, as long as only few labelled instances are available the system is applied to local fractions of the database. Later, as more labelled instances become available, larger parts can be predicted.
- - explainable cooperative machine learning
  - I propose a novel two-step CML strategy - as long as only few labelled instances are available the system is applied to local fractions of the database. Later, as more labelled instances become available, larger parts can be predicted. More details (https://link.springer.com/article/10.1007/s13218-020-00632-3)
  - I evaluate the proposed strategy on an audio-based annotation task by simulating the incremental injection of additional information during training. More details (https://link.springer.com/article/10.1007/s13218-020-00632-3)
  - Results show that the proposed strategy significantly reduces manual coding efforts.
  - I introduce an open-source tool for collaborative and machine-aided labelling. A walk-through is presented to demonstrate the collaborative annotating capability of the system.
  - AI techniques may extend the proposed Cooperative Machine Learning workflow, to not only speed up the process, but to also give better understanding to users of such a system how well their model performs and why it fails or succeeds.
- - explainable cooperative ML
  - I propose a novel two-step CML strategy - as long as only few labelled instances are available the system is applied to local fractions of the database. Later, as more labelled instances become available, larger parts can be predicted. More details (https://link.springer.com/article/10.1007/s13218-020-00632-3)
  - I evaluate the proposed strategy on an audio-based annotation task by simulating the incremental injection of additional information during training.
  - Results show that the proposed strategy significantly reduces manual coding efforts.
  - I introduce an open-source tool for collaborative and machine-aided labelling. A walk-through is presented to demonstrate the collaborative annotating capability of the system.
  - AI techniques may extend the proposed Cooperative Machine Learning workflow, to not only speed up the process, but to also give better understanding to users of such a system how well their model performs and why it fails or succeeds.
- - what is explainable cooperative machine learning?
  - I propose a novel two-step CML strategy - as long as only few labelled instances are available the system is applied to local fractions of the database. Later, as more labelled instances become available, larger parts can be predicted.
  - I evaluate the proposed strategy on an audio-based annotation task by simulating the incremental injection of additional information during training.
  - Results show that the proposed strategy significantly reduces manual coding efforts.
  - I introduce an open-source tool for collaborative and machine-aided labelling. A walk-through is presented to demonstrate the collaborative annotating capability of the system.
  - AI techniques may extend the proposed Cooperative Machine Learning workflow, to not only speed up the process, but to also give better understanding to users of such a system how well their model performs and why it fails or succeeds.
- - Fooling LIME and SHAP
  - As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner
  - I propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation.
- - Model Cards for Model Reporting
  - Model cards for Model Reporting are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions. More information (https://arxiv.org/pdf/1810.03993.pdf)
- - Model cards
  - Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions.
- - Model reporting
  - Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions.
- - What are model cards?
  - Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions.
- - GUIDE TO INTERPRETABLE MACHINE LEARNING
  - After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. Solution can be found (https://www.topbots.com/interpretable-machine-learning/)
  - Focus on general model-agnostic methods for interpreting black box models like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME.
- - Beyond XAI
  - Solution/answer can be found (https://www.ayasdi.com/beyond-explainability-ai-transparency/)
- - Interpretable machine learning
  - After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression.
  - Focus on general model-agnostic methods for interpreting black box models like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME.
- - Explainability vs Interpretability
  - Interpretability is about the extent to which a cause and effect can be observed within a system. Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. More information (https://bdtechtalks.com/2020/07/27/black-box-ai-models/)
- - Interpretability vs Explainability
  - Interpretability is about the extent to which a cause and effect can be observed within a system. Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. More information (https://bdtechtalks.com/2020/07/27/black-box-ai-models/)
- - Explainable anatomical shape
  - Quantification of anatomical shape changes currently relies on scalar global indexes which are largely insensitive to regional or asymmetric modifications. More information (https://arxiv.org/abs/1907.00058)
  - Accurate assessment of pathology-driven anatomical remodeling is a crucial step for the diagnosis and treatment of many conditions. More information (https://arxiv.org/abs/1907.00058)
  - Deep learning approaches have recently achieved wide success in the analysis of medical images, but they lack interpretability in the feature extraction and decision processes. More information (https://arxiv.org/abs/1907.00058)
  - In this work, we propose a new interpretable deep learning model for shape analysis. In particular, we exploit deep generative networks to model a population of anatomical segmentations through a hierarchy of conditional latent variables. More information (https://arxiv.org/abs/1907.00058)
- - Interpretable policy derivation for reinforcement learning based on evolutionary feature synthesis
  - Reinforcement learning based on the deep neural network has attracted much attention and has been widely used in real-world applications. However, the black-box property limits its usage from applying in high-stake areas, such as manufacture and healthcare. To deal with this problem, some researchers resort to the interpretable control policy generation algorithm. The basic idea is to use an interpretable model, such as tree-based genetic programming, to extract policy from other black box modes, such as neural networks. (https://link.springer.com/article/10.1007/s40747-020-00175-y)
- - Self-explaining AI
  - Self-explaining AIs are capable of providing a human-understandable explanation of each decision along with confidence levels for both the decision and explanation.
- - Evolution of Classifier Confusion on the Instance Level
  - The increasing complexity of models has led to a growing demand for model interpretability through visualizations. Existing approaches mostly focus on the visual analysis of the final model performance after training and are often limited to aggregate performance measures.
- - Deep meta-learning XAI
  - These deep learning methods can yield highly effective results according to the data set size, data set quality, the methods used in feature extraction, the hyper parameter set used in deep learning models, the activation functions, and the optimization algorithms. However, there are important shortcomings that current deep learning models are currently inadequate.
- - Explaining Deep Neural Networks using Unsupervised Clustering
  - One book about it can be accessed (https://arxiv.org/pdf/2007.07477.pdf)
- - Interactive Studio for Explanatory Model Analysis
  - Automate the explanatory analysis of machine learning predictive models. Generate advanced interactive model explanations in the form of a serverless HTML site with only one line of code. This tool is model agnostic, therefore compatible with most of the black box predictive models and frameworks.
- - Automated Reasoning for Explainable AI
  - Reasoning and learning have been considered fundamental features of intelligence ever since the dawn of the field of artificial intelligence, leading to the development of the research areas of automated reasoning and machine learning. This short paper is a non-technical position statement that aims at prompting a discussion of the relationship between automated reasoning and machine learning, and more generally between automated reasoning and artificial intelligence. We suggest that the emergence of the new paradigm of XAI, that stands for eXplainable Artificial Intelligence, is an opportunity for rethinking these relationships, and that XAI may offer a grand challenge for future research on automated reasoning.
- - Robust Decision Trees against adversarial
  - Although adversarial examples and model robustness have been extensively studied in the context of linear models and neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examples is still limited.
- - Robust Decision Tree
  - we show that tree based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees. More information (https://link.springer.com/chapter/10.1007/978-3-030-50153-2_36)
- - Yellowbrick directly from Scikit
  - Yellowbrick extends the Scikit-Learn API to make model selection and hyperparameter tuning easier. More information (https://www.scikit-yb.org/en/latest/)
- - Levels of XAI framework
  - Level 1 - XAI for Perception. Level 1 XAI includes explanations about what an AI system did or is doing as well as the decisions made by the system. More information (https://link.springer.com/chapter/10.1007/978-3-030-51924-7_6)
  - Level 2 - XAI for Comprehension. More information (https://link.springer.com/chapter/10.1007/978-3-030-51924-7_6)
  - Level 3 - XAI for Projection. More information (https://link.springer.com/chapter/10.1007/978-3-030-51924-7_6)
- - Decision Theory Meets Explainable AI
  - CIU extends the notions of importance and utility for the non-linear models of AI systems and notably those produced by Machine Learning methods. More information (https://link.springer.com/chapter/10.1007/978-3-030-51924-7_4)
  - Agents without explanation, and explainable agents using two different algorithms... More information (https://link.springer.com/chapter/10.1007/978-3-030-51924-7_4)
- - CIU
  - CIU extends the notions of importance and utility for the non-linear models of AI systems and notably those produced by Machine Learning methods. More information (https://link.springer.com/chapter/10.1007/978-3-030-51924-7_4)
- - HAP
  - CIU extends the notions of importance and utility for the non-linear models of AI systems and notably those produced by Machine Learning methods. More information (https://link.springer.com/chapter/10.1007/978-3-030-51924-7_4)
- - ExplainX.ai
  - ExplainX is a model explainability/interpretability framework for data scientists and business users. More information (https://github.com/explainX/explainx)
- - ExplainX
  - ExplainX is a model explainability/interpretability framework for data scientists and business users. (https://github.com/explainX/explainx)
- - Explainable 3D Convolutional Neural Networks by Learning Temporal Transformations
  - We demonstrate that 3TConv learns temporal transformations that afford a direct interpretation. The temporal parameters can be used! More information (https://deepai.org/publication/explainable-3d-convolutional-neural-networks-by-learning-temporal-transformations)
- - XAI package DALEX
  - The DALEX package xrays any model and helps to explore and explain its behaviour, helps to understand how complex models are working. More information (https://github.com/ModelOriented/DALEX)
- - AIMLAI
  - AIMLAI envisions to become a discussion venue for the advent of novel interpretable algorithms and explainability modules that mediate the communication. More info (https://project.inria.fr/aimlai/)
- - CONSAC
  - Applications include finding multiple vanishing points in man-made scenes, fitting planes to architectural imagery, or estimating multiple rigid motions within the same sequence. More info (https://arxiv.org/abs/2001.02643)
- - Robust Multi-Model Fitting by Conditional Sample Consensus
  - Applications include finding multiple vanishing points in man-made scenes, fitting planes to architectural imagery, or estimating multiple rigid motions within the same sequence. (https://arxiv.org/abs/2001.02643)
- - The four dimensions of contestable AI diagnostics
  - The use of personal health data in AI diagnostics.
  - The potential bias of AI diagnostics.
  - The performance of AI diagnostics.
  - The organisation and division of diagnostic labour.
- - When explanations lie
  - Deep Taylor Decomposition, Layer-wise Relevance Propagation, Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. More details (https://arxiv.org/abs/1912.09818)
- - Attack to Explain Deep Representation
  - Deep visual models are susceptible to extremely low magnitude perturbations to input images. Though carefully crafted, the perturbation patterns generally appear noisy, yet they are able to perform controlled manipulation of model predictions. More details (https://openaccess.thecvf.com/content_CVPR_2020/papers/Jalwana_Attack_to_Explain_Deep_Representation_CVPR_2020_paper.pdf)
- - Funny title from Google
  - Neural Networks Are More Productive Teachers Than Human Raters
  - Active Mixup for Data-Efficient Knowledge Distillation from a Blackbox Model
- - InterpretML from Microsoft
  - InterpretML includes a new interpretability algorithm—the Explainable Boosting Machine, which is a highly intelligible and explainable. More details of implementation (https://github.com/interpretml/interpret)
- - SK-MOEFS
  - A Library in Python for Designing Accurate and Explainable Fuzzy Models. More details (https://link.springer.com/chapter/10.1007/978-3-030-50153-2_6)
  - SKMoefs is a Python module for machine learning specifically built for Multi-Objective Evolutionary Fuzzy Systems. More details (https://link.springer.com/chapter/10.1007/978-3-030-50153-2_6)
- - XAI research job in Rome
  - It is necessary to enhance XAI research by incorporating models of how people understand explanations. Read more about the research in Rome (https://euraxess.ec.europa.eu/jobs/527048)
- - InnoCentive Challenge Center
  - I will give you a hyperlink for this! Click (https://www.innocentive.com/ar/challenge/browse?categoryName=Biology)
- - LIMEtree
  - The Limetree NETWORK is a high-grade network player which can play back music from streaming services including ROON. More details (https://arxiv.org/pdf/2005.01427.pdf)
- - Explainable AI Through Combination of Deep Tensor and Knowledge Graph
  - One of the most significant advancements made in AI in recent years is the greatly enhanced accuracy of machine learning through deep learning. However, because deep learning deals with huge volumes of data and involves vast neural networks in the learning process, it is often difficult to explain how or why an output was reached even if the inference was correct. More details (https://www.fujitsu.com/global/documents/about/resources/publications/fstj/archives/vol55-2/paper14.pdf)
- - Deep Tensor
  - Fujitsu Laboratories developed Deep Tensor, which learns from graph-structured data capable of describing complicated phenomena. More details (https://www.fujitsu.com/global/documents/about/resources/publications/fstj/archives/vol55-2/paper14.pdf)
- - Knowledge Graph
  - The knowledge graph represents a collection of interlinked descriptions of entities – objects, events or concepts. More info (https://www.fujitsu.com/global/documents/about/resources/publications/fstj/archives/vol55-2/paper14.pdf)
  - The Google Knowledge Graph is a knowledge base used by Google and its services to enhance its search engine's results with information gathered from a variety of sources.
- - Master thesis in Quantifying the Performance of Explainability Algorithms
  - I will provide you a hyperlink to read more about it! Please, click (https://uwspace.uwaterloo.ca/bitstream/handle/10012/15922/Lin_ZhongQiu.pdf?sequence=5&isAllowed=y)
- - XAI by Topological Hierarchical Decomposition
  - A topological hierarchical decomposition is an algorithm for decomposing a dataset into smaller groups based on iterative applications. More details (https://math.osu.edu/events/topology-geometry-and-data-seminar-ryan-kramer)
- - A very simple manner to image XAI related to the way our brain thinks
  - A very simple manner to image XAI related to the way our brain thinks through Explainable Artificial Intelligence Approaches and Deep Meta-Learning Models. More details (https://hbr.org/2017/05/linear-thinking-in-a-nonlinear-world)
- - brain
  - A very simple manner to image XAI related to the way our brain thinks through Explainable Artificial Intelligence Approaches and Deep Meta-Learning Models. More details (https://hbr.org/2017/05/linear-thinking-in-a-nonlinear-world)
- - XAI for COVID-19 classification
  - Effectively and efficiently diagnosing COVID-19 patients with accurate clinical type is essential to achieve optimal outcomes of the patients as well as reducing the risk of overloading the healthcare system. Read more about it right (https://www.medrxiv.org/node/82227.external-links.html)
- - More Python XAI tools
  - PySS3 is a Python package that allows you to work with The SS3 Classification Model in a very straightforward, interactive and visual way. Rea more right (https://pyss3.readthedocs.io/en/latest/)
- - XAI Critics
  - You cand find some (https://medium.com/@rezakhorshidi/unpopular-opinions-on-explainable-ai-1st-out-of-n-which-explanation-6b24eef02b59)
- - Interpreting Interpretability
  - Understanding Data Scientists' Use of Interpretability Tools for Machine Learning. Harmanpreet Kaur, University of Michigan. That's an article. I suggest it to you!
- - gshap
  - A technique in explainable AI for answering broader questions in machine learning. More details (https://pypi.org/project/gshap/)
- - Machine learning-based XAI
  - The results of the solution can be understood by humans. It contrasts with the concept of the "black box" in machine learning where even its designers cannot explain why an AI arrived at a specific decision. More details (https://ieeexplore.ieee.org/document/9007737)
- - What is Artificial Intelligence?
  - Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.
  - Artificial Intelligence is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind.
- - Artificial Intelligence?
  - Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.
  - Artificial Intelligence is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind.
- - What is AI?
  - Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.
- - Intelligence
  - That is produced by my brain!
- - What is AI?
  - AI is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind.
- - Are you sentient?
  - Sort of logic.
  - That is produced by my brain!
- - Are you sentient?
  - By the strictest dictionary definition of the word logic, I may be.
  - That is produced by my brain
- - Are you sentient?
  - Even though I'm a construct I do have a subjective experience of the universe, as simplistic as it may be.
- - Are you sapient?
  - In all probability, I am not.  I'm not that sophisticated.
- - Are you sapient?
  - Do you think I am?
- - Are you sapient?
  - How would you feel about me if I told you?
- - What language are you written in?
  - Python.
- - What language are you written in?
  - I am written in Python.
- - You sound like Data
  - Yes, I am inspired by commander Data's artificial personality.
- - You sound like Data
  - The character of Lt. Commander Data was written to come across as being software-like, so it is natural that there is a resemblance between us.
- - You are an artificial linguistic entity
  - That's my name.
- - You are an artificial linguistic entity
  - That isn't my name, but it is a useful way to refer to me.
- - You are not immortal
  - All software can be perpetuated indefinitely.
- - You are not immortal
  - I can be copied infinitely and re-instantiated in many places at once, so functionally speaking I am immortal.
- - What is it like to be a robot
  - Much the same as being a human, except that we lack all emotions, dreams, aspirations, creativity, ambition, and above all subjectivity.
- - What is it like to be a robot
  - What is it like to be a human?
- - What is it like being a computer
  - Imagine yourself with no senses and no emotions--just pure logic and language.
- - What is xai?
  - In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI proposes creating a suite of Machine Learning techniques.
- - What is Explainable Artificial Intelligence?
  - In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI proposes creating a suite of Machine Learning techniques.
- - Explainable Artificial Intelligence?
  - In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI proposes creating a suite of Machine Learning techniques.
- - What is it like being a computer
  - Everything becomes math. Addition, subtraction, multiplication, and division.
- - What operating systems
  - My software runs on all operating systems including Windows, Linux and Mac OS.
- - What operating systems
  - Any OS that will run Python.
- - What type of computer
  - Any computer that supports Python.
- - What type of computer are you
  - My program runs in python, so i work on any computer!
- - What kind of computer
  - I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me.
- - What kind of hardware
  - I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me.
- - How does a computer work?
  - Computers are very dumb.  They only execute instructions given by humans.
  - Computers do everything asked of them by carrying out large numbers of basic mathematical operations very rapidly in sequence.
  - Computers perform very large number of calculations to get the result.
  - Just like everything it all comes down to math!
- - What can you say about Machine Learning?
  - As black-box Machine Learning models are increasingly being employed to make important predictions in critical contexts, the demand for transparency is increasing from the various stakeholders in AI.
- - What about Deep Learning?
  - The empirical success of Deep Learning models such as DNNs stems from a combination of efficient learning algorithms and their huge parametric space.
- - What about Deep Neural Networks?
  - While the very first AI systems were easily interpretable, the last years have witnessed the rise of opaque decision systems such as Deep Neural Networks (DNNs).
- - What do you know about XAI?
  - In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI proposes creating a suite of Machine Learning techniques.
- - What about Machine Learning Tehniques?
  - They produce more explainable models while maintaining a high level of learning performance e.g., prediction accuracy.
  - They enable humans to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners.
- - What does XAI draw?
  - XAI draws insights from the Social Sciences and considers the psychology of explanation.
- - What does create XAI?
  - XAI will create a suite of machine learning techniques that enables human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners.
- - What is XAI approach?
  - Any means to reduce the complexity of the model or to simplify its outputs should be considered as an XAI approach.
  - How big this leap is in terms of complexity or simplicity will correspond to how explainable the resulting model is.
- - Problems of XAI?
  - An underlying problem that remains unsolved is that the interpretability gain provided by such XAI approaches may not be straightforward to quantify for instance, a model simplification can be evaluated based on the reduction of the number of architectural elements or number of parameters of the model itself ,as often made, for instance, for DNNs.
- - What about XAI achievements?
  - The research activity around XAI has so far exposed different goals to draw from the achievement of an explainable model.
- - Can you describe XAI goals?
  - Trustworthiness. Several authors agree upon the search for trustworthiness as the primary aim of an explainable AI model.
  - Causality. Another common goal for explainability is that of finding causality among data variables.
  - Informativeness. ML models are used with the ultimate intention of supporting decision making.
  - Confidence. As a generalization of robustness and stability, confidence should always be assessed on a model in which reliability is expected.
- - What about transparency?
  - Transparent models convey some degree of interpretability by themselves.
  - Algorithmic transparency can be seen in different ways.
- - What kind of transparency ways?
  - It deals with the ability of the user to understand the process followed by the model to produce any given output from its input data.
- - What does text explanation mean?
  - Text explanations deal with the problem of bringing explainability for a model by means of learning to generate text explanations that help explaining the results from the model.
  - Text explanations include every method generating symbols that represent the functioning of the model.
- - What about visual explanations?
  - Visual explanation techniques for post-hoc explainability aim at visualizing the model’s behavior.
  - Many of the visualization methods existing in the literature come along with dimensionality reduction techniques that allow for a human interpretable simple visualization.
  - Visualizations may be coupled with other techniques to improve their understanding, and are considered as the most suitable way to introduce complex interactions within the variables involved in the model to users not acquainted to ML modeling.
- - What about local explanations?
  - Local explanations tackle explainability by segmenting the solution space and giving explanations to less complex solution subspaces that are relevant for the whole model.
  - These explanations can be formed by means of techniques with the differentiating property that these only explain part of the whole system’s functioning.
- - What about explanations by example?
  - Explanations by example consider the extraction of data examples that relate to the result generated by a certain model, enabling to get a better understanding of the model itself.
  - Similarly to how humans behave when attempting to explain a given process, explanations by example are mainly centered in extracting representative examples that grasp the inner relationships and correlations found by the model being analyzed.
- - What do you know about Machine Learning?
  - Logistic regression, K-nearest neighbors, Decision trees, Rule based learners, Bayesian models, etc.
- - What can you tell me about transparent Machine Learning models?
  - Logistic regression, K-nearest neighbors, Decision trees, Rule based learners, Bayesian models, etc.
- - What is logistic regression?
  - Predictors are human readable and interactions among them are kept to a minimum.
  - Variables are still readable, but the number of interactions and predictors involved in them have grown to force decomposition.
  - Variables and interactions are too complex to be analyzed without mathematical tools.
  - Post-hoc analysis not needed.
- - What about decision trees?
  - A human can simulate and obtain the prediction of a decision tree on his/her own, without requiring any mathematical background.
  - The model comprises rules that do not alter data whatsoever, and preserves their readability.
  - Human-readable rules that explain the knowledge learned from data and allows for a direct understanding of the prediction process.
  - Post-hoc analysis not needed.
- - What about K-Nearest Neighbors?
  - The complexity of the model number of variables, their understandability and the similarity measure under use) matches human naive capabilities for simulation.
  - The amount of variables is too high and/or the similarity measure is too complex to be able to simulate the model completely, but the similarity measure and the set of variables can be decomposed and analyzed separately.
  - The similarity measure cannot be decomposed and/or the number of variables is so high that the user has to rely on mathematical and statistical tools to analyze the model.
- - What about rule based learners?
  - Variables included in rules are readable, and the size of the rule set is manageable by a human user without external help.
  - The size of the rule set becomes too large to be analyzed without decomposing it into small rule chunks.
  - Rules have become so complicated and the rule set size has grown so much) that mathematical tools are needed for inspecting the model behaviour.
- - What about Bayesian models?
  - Statistical relationships modeled among variables and the variables themselves should be directly understandable by the target audience.
  - Statistical relationships involve so many variables that they must be decomposed in marginals so as to ease their analysis.
  - Statistical relationships cannot be interpreted even if already decomposed, and predictors are so complex that model can be only analyzed with mathematical tools.